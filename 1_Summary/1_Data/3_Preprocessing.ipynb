{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c42684f",
   "metadata": {},
   "source": [
    "# Setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905d85b",
   "metadata": {},
   "source": [
    "## Requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eca546",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04c3af",
   "metadata": {},
   "source": [
    "## Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27614d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765de68",
   "metadata": {},
   "source": [
    "## Sample Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19026c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Episode_Title</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Publication_Day</th>\n",
       "      <th>Publication_Time</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Episode_Sentiment</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mystery Matters</td>\n",
       "      <td>Episode 98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True Crime</td>\n",
       "      <td>74.81</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Night</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>31.41998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Joke Junction</td>\n",
       "      <td>Episode 26</td>\n",
       "      <td>119.80</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>66.95</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>75.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>88.01241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Study Sessions</td>\n",
       "      <td>Episode 16</td>\n",
       "      <td>73.90</td>\n",
       "      <td>Education</td>\n",
       "      <td>69.97</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>44.92531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Digital Digest</td>\n",
       "      <td>Episode 45</td>\n",
       "      <td>67.17</td>\n",
       "      <td>Technology</td>\n",
       "      <td>57.22</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>78.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>46.27824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mind &amp; Body</td>\n",
       "      <td>Episode 86</td>\n",
       "      <td>110.51</td>\n",
       "      <td>Health</td>\n",
       "      <td>80.07</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>58.68</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>75.61031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     Podcast_Name Episode_Title  Episode_Length_minutes       Genre  \\\n",
       "0   0  Mystery Matters    Episode 98                     NaN  True Crime   \n",
       "1   1    Joke Junction    Episode 26                  119.80      Comedy   \n",
       "2   2   Study Sessions    Episode 16                   73.90   Education   \n",
       "3   3   Digital Digest    Episode 45                   67.17  Technology   \n",
       "4   4      Mind & Body    Episode 86                  110.51      Health   \n",
       "\n",
       "   Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
       "0                       74.81        Thursday            Night   \n",
       "1                       66.95        Saturday        Afternoon   \n",
       "2                       69.97         Tuesday          Evening   \n",
       "3                       57.22          Monday          Morning   \n",
       "4                       80.07          Monday        Afternoon   \n",
       "\n",
       "   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \\\n",
       "0                          NaN            0.0          Positive   \n",
       "1                        75.95            2.0          Negative   \n",
       "2                         8.97            0.0          Negative   \n",
       "3                        78.70            2.0          Positive   \n",
       "4                        58.68            3.0           Neutral   \n",
       "\n",
       "   Listening_Time_minutes  \n",
       "0                31.41998  \n",
       "1                88.01241  \n",
       "2                44.92531  \n",
       "3                46.27824  \n",
       "4                75.61031  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/podcast.csv')\n",
    "df_num = df.select_dtypes(include=[np.number])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7166404",
   "metadata": {},
   "source": [
    "# 1. Null Handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53166020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   id                           750000 non-null  int64  \n",
      " 1   Podcast_Name                 750000 non-null  object \n",
      " 2   Episode_Title                750000 non-null  object \n",
      " 3   Episode_Length_minutes       662907 non-null  float64\n",
      " 4   Genre                        750000 non-null  object \n",
      " 5   Host_Popularity_percentage   750000 non-null  float64\n",
      " 6   Publication_Day              750000 non-null  object \n",
      " 7   Publication_Time             750000 non-null  object \n",
      " 8   Guest_Popularity_percentage  603970 non-null  float64\n",
      " 9   Number_of_Ads                749999 non-null  float64\n",
      " 10  Episode_Sentiment            750000 non-null  object \n",
      " 11  Listening_Time_minutes       750000 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(6)\n",
      "memory usage: 68.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57eda70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Episode_Length_minutes</td>\n",
       "      <td>1.481068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Host_Popularity_percentage</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guest_Popularity_percentage</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number_of_Ads</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature  Importance\n",
       "0                           id    0.000000\n",
       "1       Episode_Length_minutes    1.481068\n",
       "2   Host_Popularity_percentage    0.000000\n",
       "3  Guest_Popularity_percentage    0.000000\n",
       "4                Number_of_Ads    0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "id                            -0.000876\n",
       "Episode_Length_minutes         0.916749\n",
       "Host_Popularity_percentage     0.050870\n",
       "Guest_Popularity_percentage   -0.016014\n",
       "Number_of_Ads                 -0.118337\n",
       "Listening_Time_minutes         1.000000\n",
       "Name: Listening_Time_minutes, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'Number_of_Ads' : Only one -> just drop the row.\n",
    "print(df['Number_of_Ads'].isnull().sum())\n",
    "df = df.dropna(subset=['Number_of_Ads'])\n",
    "\n",
    "# 'Episode_Length_minutes' and 'Guest_Popularity_percentage' : Large number of nulls.\n",
    "# Let's briefly check the feature importance.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "X = df_num.drop(columns=['Listening_Time_minutes'])\n",
    "y = df_num['Listening_Time_minutes']\n",
    "features = X.columns\n",
    "\n",
    "model = RandomForestRegressor(max_leaf_nodes    = 10, \n",
    "                              max_depth         = 3, \n",
    "                              n_estimators      = 5,\n",
    "                              random_state      = 42,)\n",
    "model.fit(X, y)\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "perm = permutation_importance(model, X, y, n_repeats=2, random_state=42)\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': perm.importances_mean})\n",
    "\n",
    "display(importance_df)\n",
    "\n",
    "display(df_num.corr()['Listening_Time_minutes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 662906 entries, 1 to 749999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   id                           662906 non-null  int64  \n",
      " 1   Podcast_Name                 662906 non-null  object \n",
      " 2   Episode_Title                662906 non-null  object \n",
      " 3   Episode_Length_minutes       662906 non-null  float64\n",
      " 4   Genre                        662906 non-null  object \n",
      " 5   Host_Popularity_percentage   662906 non-null  float64\n",
      " 6   Publication_Day              662906 non-null  object \n",
      " 7   Publication_Time             662906 non-null  object \n",
      " 8   Guest_Popularity_percentage  662906 non-null  float64\n",
      " 9   Number_of_Ads                662906 non-null  float64\n",
      " 10  Episode_Sentiment            662906 non-null  object \n",
      " 11  Listening_Time_minutes       662906 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(6)\n",
      "memory usage: 65.7+ MB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 'Episode_Length_minutes' seems to be important -> delete rows with nulls.\n",
    "df = df.dropna(subset=['Episode_Length_minutes'])\n",
    "\n",
    "# 'Guest_Popularity_percentage' seems to be not important -> fill nulls with mean.\n",
    "df['Guest_Popularity_percentage'] = df['Guest_Popularity_percentage'].fillna(df['Guest_Popularity_percentage'].mean())\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1e0ed",
   "metadata": {},
   "source": [
    "# 2. Feature Scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea78c5",
   "metadata": {},
   "source": [
    "# 3. Encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b824e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d61e936",
   "metadata": {},
   "source": [
    "# 4. Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction maps features from a high-dimensional space to a lower-dimensional one while preserving as much task-relevant structure as possible. It helps with:\n",
    "- Curse of dimensionality and collinearity reduction.\n",
    "- Speeding up training/inference and mitigating overfitting.\n",
    "- Visualization and exploratory analysis.\n",
    "- Noise suppression by projecting onto informative subspaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e2c48",
   "metadata": {},
   "source": [
    "## 4.1 What & When to Use\n",
    "Use dimensionality reduction when you have many features relative to samples, strong feature correlations, sparse/high-cardinality encodings, or you need 2D/3D visualization.\n",
    "\n",
    "**Families:**\n",
    "- **Linear projections:** PCA, TruncatedSVD, RandomProjection (Gaussian/Sparse). Good first-line choices; fast and scalable.\n",
    "- **Manifold learning:** UMAP, t-SNE. Nonlinear; preserve local neighborhoods for visualization; not ideal inside production pipelines that require stable transforms across fits (especially t-SNE).\n",
    "- **Clustering of features:** FeatureAgglomeration merges similar features.\n",
    "- **Neural:** Autoencoders learn non-linear compressions; require more engineering and compute.\n",
    "\n",
    "**Choosing tips:**\n",
    "- Dense numeric data → Standardize → PCA.\n",
    "- Sparse text/categorical (e.g., TF-IDF, One-Hot) → TruncatedSVD.\n",
    "- Very high dimensional with limited time → RandomProjection.\n",
    "- Visualization or local structure → UMAP (often) or t-SNE.\n",
    "- Want human interpretation of components → PCA (loadings, explained variance).\n",
    "- Need end-to-end learnable compression → Autoencoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399eae12",
   "metadata": {},
   "source": [
    "## 4.2 Methods at a Glance\n",
    "- **PCA (Principal Component Analysis):** Orthogonal linear components maximizing variance. Sensitive to scaling; use after StandardScaler. Outputs `explained_variance_ratio_` to pick `n_components` (e.g., 90–99%).\n",
    "- **TruncatedSVD:** PCA-like on sparse matrices without centering. Works with TF-IDF / One-Hot; choose components via held-out downstream score.\n",
    "- **Random Projections (Gaussian / Sparse):** Johnson–Lindenstrauss lemma for distance preservation with O(ndk). No fitting cost; set `n_components` to hundreds–thousands depending on samples.\n",
    "- **UMAP:** Fast manifold learner; preserves local neighborhoods. Good for visualization and as a preprocessor if you freeze a fitted model.\n",
    "- **t-SNE:** High-quality 2D/3D visualization; not typically used inside ML pipelines (non-parametric, expensive, sensitive to hyperparameters).\n",
    "- **FeatureAgglomeration:** Clusters features (columns) using hierarchical clustering; outputs averaged clusters as features.\n",
    "- **Autoencoders:** Neural network encoder-decoder minimizing reconstruction error; flexible but requires tuning and GPU for scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7f489",
   "metadata": {},
   "source": [
    "## 4.3 How to Evaluate\n",
    "- **Supervised objective:** Compare cross-validated downstream scores *with vs. without* reduction. Choose `n_components` that maximizes validation score.\n",
    "- **Unsupervised structural faithfulness:**\n",
    "  - **Trustworthiness** / **Continuity** (neighborhood preservation) for embeddings.\n",
    "  - **Reconstruction error** for PCA/Autoencoders.\n",
    "- **Stability & reproducibility:** Fix `random_state`, record seeds, version transforms, and persist fitted reducers.\n",
    "- **Runtime & memory:** Measure transform time and peak memory to balance accuracy and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f5a29",
   "metadata": {},
   "source": [
    "## 4.4 Practical Tips & Pitfalls\n",
    "- Always place reducers **inside** scikit-learn Pipelines to avoid leakage.\n",
    "- Standardize dense numeric features before PCA; **do not center** sparse matrices before TruncatedSVD.\n",
    "- For UMAP/t-SNE, start with `n_neighbors≈min(15, sqrt(n_samples))` and check trustworthiness.\n",
    "- For interpretability, inspect PCA loadings (top absolute coefficients per component) and plot cumulative explained variance.\n",
    "- Keep the reducer fitted on training data and reuse it for validation/test to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edd24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA+LR</td>\n",
       "      <td>0.990491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TruncatedSVD+LR</td>\n",
       "      <td>0.433172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GaussianRP+LR</td>\n",
       "      <td>0.972876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t-SNE (trustworthiness)</td>\n",
       "      <td>0.795987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    method     score\n",
       "0                   PCA+LR  0.990491\n",
       "1          TruncatedSVD+LR  0.433172\n",
       "2            GaussianRP+LR  0.972876\n",
       "3  t-SNE (trustworthiness)  0.795987"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from sklearn.manifold import TSNE, trustworthiness\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, silhouette_score\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Example dense dataset.\n",
    "X_dense = np.random.RandomState(RANDOM_STATE).randn(1000, 50)\n",
    "y = (X_dense[:, 0] + 0.5*X_dense[:, 1] + 0.1*np.random.RandomState(RANDOM_STATE).randn(1000) > 0).astype(int)\n",
    "\n",
    "# Example sparse dataset (simulating TF-IDF-like inputs).\n",
    "X_sparse = csr_matrix(np.random.RandomState(RANDOM_STATE).poisson(0.05, size=(1000, 5000)))\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_dense, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# 1) PCA pipeline on dense numeric data.\n",
    "pipe_pca = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"pca\", PCA(random_state=RANDOM_STATE)),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, n_jobs=None, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_pca = {\"pca__n_components\": [0.8, 0.9, 0.95, 0.99, 10, 20, 30]}  # ratio or integer.\n",
    "g_pca = GridSearchCV(pipe_pca, param_grid=param_pca, cv=5, n_jobs=-1, scoring=\"roc_auc\")\n",
    "g_pca.fit(Xtr, ytr)\n",
    "pca_auc = roc_auc_score(yte, g_pca.predict_proba(Xte)[:, 1])\n",
    "\n",
    "# 2) TruncatedSVD for sparse high-dimensional inputs.\n",
    "pipe_tsvd = Pipeline([\n",
    "    (\"tsvd\", TruncatedSVD(random_state=RANDOM_STATE)),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, n_jobs=None, random_state=RANDOM_STATE))\n",
    "])\n",
    "param_tsvd = {\"tsvd__n_components\": [50, 100, 200, 300]}\n",
    "g_tsvd = GridSearchCV(pipe_tsvd, param_grid=param_tsvd, cv=5, n_jobs=-1, scoring=\"roc_auc\")\n",
    "g_tsvd.fit(X_sparse[:800], y[:800])\n",
    "tsvd_auc = roc_auc_score(y[800:], g_tsvd.predict_proba(X_sparse[800:])[:, 1])\n",
    "\n",
    "# 3) Random projection (Gaussian) as a fast baseline.\n",
    "pipe_rp = Pipeline([\n",
    "    (\"rp\", GaussianRandomProjection(n_components=50, random_state=RANDOM_STATE)),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, n_jobs=None, random_state=RANDOM_STATE))\n",
    "])\n",
    "pipe_rp.fit(Xtr, ytr)\n",
    "rp_auc = roc_auc_score(yte, pipe_rp.predict_proba(Xte)[:, 1])\n",
    "\n",
    "# 4) FeatureAgglomeration to merge similar columns.\n",
    "agg = FeatureAgglomeration(n_clusters=20)  # Choose clusters based on CV score.\n",
    "Xtr_agg = agg.fit_transform(Xtr)\n",
    "Xte_agg = agg.transform(Xte)\n",
    "\n",
    "# 5) UMAP / t-SNE for visualization-like embeddings.\n",
    "# Note: UMAP is not in sklearn; if available, use: from umap import UMAP\n",
    "# Here we use t-SNE to produce a 2D embedding and measure trustworthiness.\n",
    "# Fit on a subset for speed.\n",
    "subset = np.random.RandomState(RANDOM_STATE).choice(X_dense.shape[0], size=500, replace=False)\n",
    "X_sub = X_dense[subset]\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"pca\", random_state=RANDOM_STATE)\n",
    "X_emb = tsne.fit_transform(X_sub)\n",
    "\n",
    "tw = trustworthiness(X_sub, X_emb, n_neighbors=5)\n",
    "\n",
    "# Summarize results.\n",
    "summary = pd.DataFrame({\n",
    "    \"method\": [\"PCA+LR\", \"TruncatedSVD+LR\", \"GaussianRP+LR\", \"t-SNE (trustworthiness)\"],\n",
    "    \"score\": [pca_auc, tsvd_auc, rp_auc, tw]\n",
    "})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f667845",
   "metadata": {},
   "source": [
    "# 5. Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f05c0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yana_library",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba89d98",
   "metadata": {},
   "source": [
    "# 0. Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c848b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: numpy in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 2)) (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 3)) (2.3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 4)) (3.10.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 5)) (1.7.1)\n",
      "Collecting torch==2.7.0 (from -r ../../requirements_ai-prac.txt (line 9))\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp310-cp310-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision==0.22.0 (from -r ../../requirements_ai-prac.txt (line 10))\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio==2.7.0 (from -r ../../requirements_ai-prac.txt (line 11))\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 13)) (4.56.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 14)) (1.10.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 15)) (6.32.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 16)) (0.11.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 17)) (0.2.1)\n",
      "Requirement already satisfied: hf_xet in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from -r ../../requirements_ai-prac.txt (line 18)) (1.1.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from torch==2.7.0->-r ../../requirements_ai-prac.txt (line 9)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from torch==2.7.0->-r ../../requirements_ai-prac.txt (line 9)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from torch==2.7.0->-r ../../requirements_ai-prac.txt (line 9)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from torch==2.7.0->-r ../../requirements_ai-prac.txt (line 9)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from torch==2.7.0->-r ../../requirements_ai-prac.txt (line 9)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from torch==2.7.0->-r ../../requirements_ai-prac.txt (line 9)) (2025.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from torchvision==0.22.0->-r ../../requirements_ai-prac.txt (line 10)) (11.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from pandas->-r ../../requirements_ai-prac.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from pandas->-r ../../requirements_ai-prac.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from pandas->-r ../../requirements_ai-prac.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from matplotlib->-r ../../requirements_ai-prac.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from matplotlib->-r ../../requirements_ai-prac.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from matplotlib->-r ../../requirements_ai-prac.txt (line 4)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from matplotlib->-r ../../requirements_ai-prac.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from matplotlib->-r ../../requirements_ai-prac.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from matplotlib->-r ../../requirements_ai-prac.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from scikit-learn->-r ../../requirements_ai-prac.txt (line 5)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from scikit-learn->-r ../../requirements_ai-prac.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from scikit-learn->-r ../../requirements_ai-prac.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from transformers->-r ../../requirements_ai-prac.txt (line 13)) (0.34.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from transformers->-r ../../requirements_ai-prac.txt (line 13)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from transformers->-r ../../requirements_ai-prac.txt (line 13)) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from transformers->-r ../../requirements_ai-prac.txt (line 13)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from transformers->-r ../../requirements_ai-prac.txt (line 13)) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from transformers->-r ../../requirements_ai-prac.txt (line 13)) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from transformers->-r ../../requirements_ai-prac.txt (line 13)) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from accelerate->-r ../../requirements_ai-prac.txt (line 14)) (7.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r ../../requirements_ai-prac.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from requests->transformers->-r ../../requirements_ai-prac.txt (line 13)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from requests->transformers->-r ../../requirements_ai-prac.txt (line 13)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from requests->transformers->-r ../../requirements_ai-prac.txt (line 13)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from requests->transformers->-r ../../requirements_ai-prac.txt (line 13)) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.0->-r ../../requirements_ai-prac.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from tqdm>=4.27->transformers->-r ../../requirements_ai-prac.txt (line 13)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yana\\anaconda3\\envs\\ai-prac\\lib\\site-packages (from jinja2->torch==2.7.0->-r ../../requirements_ai-prac.txt (line 9)) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp310-cp310-win_amd64.whl (2770.9 MB)\n",
      "   ---------------------------------------- 0.0/2.8 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.8 GB 109.2 MB/s eta 0:00:26\n",
      "    --------------------------------------- 0.0/2.8 GB 106.2 MB/s eta 0:00:26\n",
      "    --------------------------------------- 0.1/2.8 GB 103.5 MB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.1/2.8 GB 105.9 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.1/2.8 GB 104.6 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.1/2.8 GB 105.2 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 0.2/2.8 GB 104.6 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 0.2/2.8 GB 104.8 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.2/2.8 GB 105.2 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 0.2/2.8 GB 105.5 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 0.2/2.8 GB 105.9 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 0.3/2.8 GB 107.5 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 108.1 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 109.6 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 109.5 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 109.5 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 109.6 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 110.3 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.4/2.8 GB 110.3 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.5/2.8 GB 109.5 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.5/2.8 GB 108.8 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 108.1 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 107.5 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 0.5/2.8 GB 105.4 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 0.6/2.8 GB 104.8 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 0.6/2.8 GB 104.1 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 0.6/2.8 GB 104.1 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 0.6/2.8 GB 103.5 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 0.7/2.8 GB 102.8 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 0.7/2.8 GB 102.8 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 102.8 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 103.4 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 103.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 103.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 104.1 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 104.1 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 104.1 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 0.9/2.8 GB 103.5 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 0.9/2.8 GB 102.8 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 0.9/2.8 GB 102.2 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 0.9/2.8 GB 102.2 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 0.9/2.8 GB 102.8 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 1.0/2.8 GB 101.0 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 1.0/2.8 GB 101.6 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 1.0/2.8 GB 101.6 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 1.0/2.8 GB 102.2 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 1.0/2.8 GB 102.2 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 1.1/2.8 GB 97.5 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 1.1/2.8 GB 94.7 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 1.1/2.8 GB 94.2 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 94.7 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 94.7 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 94.7 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 95.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 94.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 94.7 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 94.2 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 1.3/2.8 GB 93.1 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 1.3/2.8 GB 92.1 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 1.3/2.8 GB 90.6 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 1.3/2.8 GB 96.3 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 1.3/2.8 GB 96.3 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 1.4/2.8 GB 96.9 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 1.4/2.8 GB 96.9 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 1.4/2.8 GB 96.3 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 1.4/2.8 GB 95.8 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 1.4/2.8 GB 94.7 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 1.5/2.8 GB 94.1 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 1.5/2.8 GB 92.6 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 1.5/2.8 GB 93.1 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 1.5/2.8 GB 93.1 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 94.2 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 95.8 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 96.3 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 96.9 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 96.9 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 97.5 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 1.7/2.8 GB 99.8 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 101.6 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 105.4 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 1.7/2.8 GB 107.5 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 1.8/2.8 GB 108.8 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 1.8/2.8 GB 110.3 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 1.8/2.8 GB 111.0 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 1.8/2.8 GB 112.5 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 1.9/2.8 GB 112.5 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 1.9/2.8 GB 114.0 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 1.9/2.8 GB 114.0 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 114.8 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 114.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 114.8 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 114.8 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 114.8 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 114.8 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 2.1/2.8 GB 114.0 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 2.1/2.8 GB 107.5 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 2.1/2.8 GB 104.8 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 2.1/2.8 GB 104.8 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 104.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.2/2.8 GB 104.8 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.2/2.8 GB 104.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 2.2/2.8 GB 104.8 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 2.3/2.8 GB 104.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 2.3/2.8 GB 102.8 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.3/2.8 GB 101.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.3/2.8 GB 99.8 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.3/2.8 GB 99.2 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 108.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 107.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 106.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 105.4 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 104.8 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 102.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 101.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.5/2.8 GB 99.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.5/2.8 GB 99.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.6/2.8 GB 100.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.6/2.8 GB 101.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 2.6/2.8 GB 101.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 2.6/2.8 GB 100.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.6/2.8 GB 100.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 100.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 102.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.8 GB 102.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.8 GB 103.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 105.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 GB 48.0 MB/s  0:00:29\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp310-cp310-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 2.9/6.3 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 8.1 MB/s  0:00:01\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.2/4.2 MB 62.8 MB/s  0:00:00\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\n",
      "  Attempting uninstall: torch\n",
      "\n",
      "    Found existing installation: torch 2.8.0\n",
      "\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "    Uninstalling torch-2.8.0:\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "      Successfully uninstalled torch-2.8.0\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   ---------------------------------------- 3/3 [torchaudio]\n",
      "\n",
      "Successfully installed torch-2.7.0+cu126 torchaudio-2.7.0+cu126 torchvision-0.22.0+cu126\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\yana\\anaconda3\\envs\\ai-prac\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../../requirements_ai-prac.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d44636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9cdc9",
   "metadata": {},
   "source": [
    "# 1. Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6297a14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>body</th>\n",
       "      <th>rule</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>positive_example_1</th>\n",
       "      <th>positive_example_2</th>\n",
       "      <th>negative_example_1</th>\n",
       "      <th>negative_example_2</th>\n",
       "      <th>rule_violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Banks don't want you to know this! Click here ...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>Futurology</td>\n",
       "      <td>If you could tell your younger self something ...</td>\n",
       "      <td>hunt for lady for jack off in neighbourhood ht...</td>\n",
       "      <td>Watch Golden Globe Awards 2017 Live Online in ...</td>\n",
       "      <td>DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SD Stream [ ENG Link 1] (http://www.sportsstre...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>soccerstreams</td>\n",
       "      <td>[I wanna kiss you all over! Stunning!](http://...</td>\n",
       "      <td>LOLGA.COM is One of the First Professional Onl...</td>\n",
       "      <td>#Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...</td>\n",
       "      <td>[15 Amazing Hidden Features Of Google Search Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Lol. Try appealing the ban and say you won't d...</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>Don't break up with him or call the cops.  If ...</td>\n",
       "      <td>It'll be dismissed: https://en.wikipedia.org/w...</td>\n",
       "      <td>Where is there a site that still works where y...</td>\n",
       "      <td>Because this statement of his is true. It isn'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she will come your home open her legs with  an...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>sex</td>\n",
       "      <td>Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...</td>\n",
       "      <td>tight pussy watch for your cock get her at thi...</td>\n",
       "      <td>NSFW(obviously) http://spankbang.com/iy3u/vide...</td>\n",
       "      <td>Good News ::Download WhatsApp 2.16.230 APK for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>code free tyrande ---&gt;&gt;&gt; [Imgur](http://i.imgu...</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "      <td>hearthstone</td>\n",
       "      <td>wow!! amazing reminds me of the old days.Well...</td>\n",
       "      <td>seek for lady for sex in around http://p77.pl/...</td>\n",
       "      <td>must be watch movie https://sites.google.com/s...</td>\n",
       "      <td>We're streaming Pokemon Veitnamese Crystal RIG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                                               body  \\\n",
       "0       0  Banks don't want you to know this! Click here ...   \n",
       "1       1  SD Stream [ ENG Link 1] (http://www.sportsstre...   \n",
       "2       2  Lol. Try appealing the ban and say you won't d...   \n",
       "3       3  she will come your home open her legs with  an...   \n",
       "4       4  code free tyrande --->>> [Imgur](http://i.imgu...   \n",
       "\n",
       "                                                rule      subreddit  \\\n",
       "0  No Advertising: Spam, referral links, unsolici...     Futurology   \n",
       "1  No Advertising: Spam, referral links, unsolici...  soccerstreams   \n",
       "2  No legal advice: Do not offer or request legal...   pcmasterrace   \n",
       "3  No Advertising: Spam, referral links, unsolici...            sex   \n",
       "4  No Advertising: Spam, referral links, unsolici...    hearthstone   \n",
       "\n",
       "                                  positive_example_1  \\\n",
       "0  If you could tell your younger self something ...   \n",
       "1  [I wanna kiss you all over! Stunning!](http://...   \n",
       "2  Don't break up with him or call the cops.  If ...   \n",
       "3  Selling Tyrande codes for 3â‚¬ to paypal. PM. \\n...   \n",
       "4   wow!! amazing reminds me of the old days.Well...   \n",
       "\n",
       "                                  positive_example_2  \\\n",
       "0  hunt for lady for jack off in neighbourhood ht...   \n",
       "1  LOLGA.COM is One of the First Professional Onl...   \n",
       "2  It'll be dismissed: https://en.wikipedia.org/w...   \n",
       "3  tight pussy watch for your cock get her at thi...   \n",
       "4  seek for lady for sex in around http://p77.pl/...   \n",
       "\n",
       "                                  negative_example_1  \\\n",
       "0  Watch Golden Globe Awards 2017 Live Online in ...   \n",
       "1  #Rapper \\nðŸš¨Straight Outta Cross Keys SC ðŸš¨YouTu...   \n",
       "2  Where is there a site that still works where y...   \n",
       "3  NSFW(obviously) http://spankbang.com/iy3u/vide...   \n",
       "4  must be watch movie https://sites.google.com/s...   \n",
       "\n",
       "                                  negative_example_2  rule_violation  \n",
       "0  DOUBLE CEE x BANDS EPPS - \"BIRDS\"\\n\\nDOWNLOAD/...               0  \n",
       "1  [15 Amazing Hidden Features Of Google Search Y...               0  \n",
       "2  Because this statement of his is true. It isn'...               1  \n",
       "3  Good News ::Download WhatsApp 2.16.230 APK for...               1  \n",
       "4  We're streaming Pokemon Veitnamese Crystal RIG...               1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2340a581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Banks don't want you to know this! Click here to know more!\n",
      "No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Futurology\n",
      "If you could tell your younger self something different about sex, what would that be?\n",
      "\n",
      "i AM IN A CONTEST TO WIN FUNDING FOR MY SEX POSITIVE FILM: VOTE HERE:\n",
      "\n",
      "http://sheknows.offerpop.com/campaign/813112/entry/v144417\n",
      "hunt for lady for jack off in neighbourhood http://url.inmusi.com/gakq\n",
      "Watch Golden Globe Awards 2017 Live Online in HD Coverage without ADS (VIP STREAMS)\n",
      "=\n",
      "\n",
      "HD STREAM QUALITY >>> [WATCH LINK1](http://forum.submitexpress.com/viewtopic.php?f=9&t=215858)\n",
      "=\n",
      "\n",
      "HD BROADCASTING QUALITY >>> [WATCH LINK1](http://forum.submitexpress.com/viewtopic.php?f=9&t=215858)\n",
      "=\n",
      "\n",
      "Mobile Compatibility: YES\n",
      "=\n",
      "\n",
      "NO ADS | NO ADS | ADS\n",
      "=\n",
      "\n",
      "DOUBLE CEE x BANDS EPPS - \"BIRDS\"\n",
      "\n",
      "DOWNLOAD/STREAM:\n",
      "\n",
      "http://music.theblacksmithed.com/download/birds/\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for r in df.iloc[0]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cdc27",
   "metadata": {},
   "source": [
    "# 2. Simple Trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c5ee2",
   "metadata": {},
   "source": [
    "## 2.1. `tf-idf` and `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "921f198d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF+LR fold AUCs: [0.8389 0.7741 0.8309 0.8419 0.8003] | mean=0.8172.\n",
      "Saved TF-IDF artifacts to ./artifacts/.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: TF-IDF + Logistic Regression. Saves OOF and test predictions for blending.\n",
    "# - Place data under ./data or the current directory; auto-detection included.\n",
    "\n",
    "import os, gc, random, warnings, json\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as SkLR\n",
    "\n",
    "# Reproducibility.\n",
    "SEED = 42\n",
    "def seed_all(s=SEED):\n",
    "    random.seed(s); np.random.seed(s); os.environ[\"PYTHONHASHSEED\"] = str(s)\n",
    "seed_all()\n",
    "\n",
    "# Locate data.\n",
    "CAND_DATA_DIRS = [Path(\"data\"), Path(\".\"), Path(\"./jigsaw-agile-community-rules\")]\n",
    "DATA_DIR = next((p for p in CAND_DATA_DIRS if (p / \"train.csv\").exists() and (p / \"test.csv\").exists()), None)\n",
    "assert DATA_DIR is not None, \"Could not find train.csv and test.csv under ./data, . or ./jigsaw-agile-community-rules.\"\n",
    "\n",
    "# Read data.\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\").reset_index(drop=True)\n",
    "test  = pd.read_csv(DATA_DIR / \"test.csv\").reset_index(drop=True)\n",
    "sub   = pd.read_csv(DATA_DIR / \"sample_submission.csv\")\n",
    "\n",
    "# Column picking.\n",
    "TEXT_CANDS = [\"body\", \"comment_text\", \"text\", \"comment\"]\n",
    "RULE_CANDS = [\"rule\", \"rule_text\", \"rule_description\"]\n",
    "SUBR_CANDS = [\"subreddit\", \"subreddit_name\", \"community\"]\n",
    "LABEL_CANDS = [\"rule_violation\", \"target\", \"label\"]\n",
    "\n",
    "def pick_col(df, cands, required=True):\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"Missing columns: need any of {cands}. Columns present: {list(df.columns)}.\")\n",
    "    return None\n",
    "\n",
    "TEXT_COL  = pick_col(train, TEXT_CANDS)\n",
    "RULE_COL  = pick_col(train, RULE_CANDS)\n",
    "SUBR_COL  = pick_col(train, SUBR_CANDS)\n",
    "LABEL_COL = pick_col(train, LABEL_CANDS)\n",
    "\n",
    "# Rule-conditioned text.\n",
    "def compose(df):\n",
    "    return (\n",
    "        df[RULE_COL].astype(str).fillna(\"\") + \" [SEP] \" +\n",
    "        df[SUBR_COL].astype(str).fillna(\"\") + \" [SEP] \" +\n",
    "        df[TEXT_COL].astype(str).fillna(\"\")\n",
    "    )\n",
    "\n",
    "train[\"text_cond\"] = compose(train)\n",
    "test[\"text_cond\"]  = compose(test)\n",
    "y = train[LABEL_COL].astype(int).values\n",
    "\n",
    "# 5-fold stratified CV.\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "folds = list(skf.split(np.zeros(len(train)), y))\n",
    "\n",
    "# TF-IDF + Logistic Regression.\n",
    "tfidf = TfidfVectorizer(max_features=200_000, ngram_range=(1, 2), min_df=2)\n",
    "X_all  = tfidf.fit_transform(train[\"text_cond\"].tolist())\n",
    "X_test = tfidf.transform(test[\"text_cond\"].tolist())\n",
    "\n",
    "oof_lr = np.zeros(len(train), dtype=float)\n",
    "test_pred_lr = np.zeros(len(test), dtype=float)\n",
    "auc_lr_folds = []\n",
    "\n",
    "for fold, (tr, va) in enumerate(folds, 1):\n",
    "    clf = SkLR(max_iter=1000, C=4.0, class_weight=\"balanced\", solver=\"liblinear\")\n",
    "    clf.fit(X_all[tr], y[tr])\n",
    "    oof_lr[va] = clf.predict_proba(X_all[va])[:, 1]\n",
    "    auc = roc_auc_score(y[va], oof_lr[va])\n",
    "    auc_lr_folds.append(auc)\n",
    "    test_pred_lr += clf.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "\n",
    "print(f\"TF-IDF+LR fold AUCs: {np.round(auc_lr_folds, 4)} | mean={np.mean(auc_lr_folds):.4f}.\")\n",
    "\n",
    "# Save artifacts so Cell 2 can blend, even if run in a fresh kernel.\n",
    "ARTIFACTS = Path(\"./artifacts\"); ARTIFACTS.mkdir(exist_ok=True, parents=True)\n",
    "np.save(ARTIFACTS / \"oof_lr.npy\", oof_lr)\n",
    "np.save(ARTIFACTS / \"test_pred_lr.npy\", test_pred_lr)\n",
    "meta = {\n",
    "    \"SEED\": SEED, \"N_FOLDS\": N_FOLDS,\n",
    "    \"TEXT_COL\": TEXT_COL, \"RULE_COL\": RULE_COL, \"SUBR_COL\": SUBR_COL, \"LABEL_COL\": LABEL_COL,\n",
    "    \"train_len\": int(len(train)), \"test_len\": int(len(test))\n",
    "}\n",
    "with open(ARTIFACTS / \"meta.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Saved TF-IDF artifacts to ./artifacts/.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e228da",
   "metadata": {},
   "source": [
    "## 2.2. DeBERTaâ€‘v3â€‘large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135e317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA=True, CC=12, bf16=True, fp16=False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Model device: cuda:0, dtype: torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 00:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692571</td>\n",
       "      <td>0.668192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.691114</td>\n",
       "      <td>0.666505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2] Model device: cuda:0, dtype: torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 00:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.679524</td>\n",
       "      <td>0.616687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.681200</td>\n",
       "      <td>0.677753</td>\n",
       "      <td>0.623859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3] Model device: cuda:0, dtype: torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 00:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.683933</td>\n",
       "      <td>0.608568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.683029</td>\n",
       "      <td>0.611578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4] Model device: cuda:0, dtype: torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 00:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.638349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.684200</td>\n",
       "      <td>0.670531</td>\n",
       "      <td>0.641480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5] Model device: cuda:0, dtype: torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 00:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.681870</td>\n",
       "      <td>0.598795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>0.680616</td>\n",
       "      <td>0.605064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeBERTa fold AUCs: [0.5014 0.4953 0.516  0.5025 0.5156] | mean=0.5062.\n",
      "Blended OOF AUC: 0.8174.\n",
      "Wrote submission.csv with shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: DeBERTa-v3-large fine-tune, then calibration + blending + submission.\n",
    "# - Uses TF-IDF artifacts from memory or loads them from ./artifacts/.\n",
    "\n",
    "import os, gc, random, warnings, json\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression as LRCal\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Reproducibility and performance.\n",
    "# --------------------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "def seed_all(s=SEED):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(s)\n",
    "seed_all()\n",
    "\n",
    "IS_WINDOWS = (os.name == \"nt\")\n",
    "\n",
    "# Precision selector (mutually exclusive).\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "CC_MAJOR = torch.cuda.get_device_capability(0)[0] if IS_CUDA else 0\n",
    "USE_BF16 = IS_CUDA and CC_MAJOR >= 8   # Ampere or newer.\n",
    "USE_FP16 = IS_CUDA and not USE_BF16    # Pre-Ampere.\n",
    "\n",
    "if IS_CUDA:\n",
    "    # Safe speed boosts on NVIDIA GPUs.\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(f\"CUDA={IS_CUDA}, CC={CC_MAJOR if IS_CUDA else 'NA'}, bf16={USE_BF16}, fp16={USE_FP16}.\")\n",
    "\n",
    "# DataLoader workers: use 0 on Windows notebooks to avoid pickling issues.\n",
    "DATALOADER_WORKERS = 0 if IS_WINDOWS else 4\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Load data.\n",
    "# --------------------------------------------------------------------------------------\n",
    "def load_data():\n",
    "    CAND_DATA_DIRS = [Path(\"data\"), Path(\".\"), Path(\"./jigsaw-agile-community-rules\")]\n",
    "    data_dir = next((p for p in CAND_DATA_DIRS if (p / \"train.csv\").exists() and (p / \"test.csv\").exists()), None)\n",
    "    assert data_dir is not None, \"Could not find train.csv and test.csv under ./data, . or ./jigsaw-agile-community-rules.\"\n",
    "    train = pd.read_csv(data_dir / \"train.csv\").reset_index(drop=True)\n",
    "    test  = pd.read_csv(data_dir / \"test.csv\").reset_index(drop=True)\n",
    "    sub   = pd.read_csv(data_dir / \"sample_submission.csv\")\n",
    "    return train, test, sub\n",
    "\n",
    "if \"train\" not in globals() or \"test\" not in globals():\n",
    "    train, test, sub = load_data()\n",
    "\n",
    "# Column picking consistent with Cell 1.\n",
    "TEXT_CANDS = [\"body\", \"comment_text\", \"text\", \"comment\"]\n",
    "RULE_CANDS = [\"rule\", \"rule_text\", \"rule_description\"]\n",
    "SUBR_CANDS = [\"subreddit\", \"subreddit_name\", \"community\"]\n",
    "LABEL_CANDS = [\"rule_violation\", \"target\", \"label\"]\n",
    "\n",
    "def pick_col(df, cands, required=True):\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"Missing columns: need any of {cands}. Columns present: {list(df.columns)}.\")\n",
    "    return None\n",
    "\n",
    "TEXT_COL  = pick_col(train, TEXT_CANDS)\n",
    "RULE_COL  = pick_col(train, RULE_CANDS)\n",
    "SUBR_COL  = pick_col(train, SUBR_CANDS)\n",
    "LABEL_COL = pick_col(train, LABEL_CANDS)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Tokenizer and collator.\n",
    "# --------------------------------------------------------------------------------------\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "SEP_TOKEN = tokenizer.sep_token or \"[SEP]\"\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")  # Ensure PyTorch tensors.\n",
    "\n",
    "# Compose input text; keep simple, or swap SEP_TOKEN with actual special token.\n",
    "def compose(df):\n",
    "    return (\n",
    "        df[RULE_COL].astype(str).fillna(\"\") + f\" {SEP_TOKEN} \" +\n",
    "        df[SUBR_COL].astype(str).fillna(\"\") + f\" {SEP_TOKEN} \" +\n",
    "        df[TEXT_COL].astype(str).fillna(\"\")\n",
    "    )\n",
    "\n",
    "if \"text_cond\" not in train.columns:\n",
    "    train[\"text_cond\"] = compose(train)\n",
    "if \"text_cond\" not in test.columns:\n",
    "    test[\"text_cond\"] = compose(test)\n",
    "\n",
    "y = train[LABEL_COL].astype(int).values\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# CV splits.\n",
    "# --------------------------------------------------------------------------------------\n",
    "N_FOLDS = 3\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "folds = list(skf.split(np.zeros(len(train)), y))\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Dataset.\n",
    "# --------------------------------------------------------------------------------------\n",
    "class DS(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts; self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = tokenizer(self.texts[i], truncation=True, max_length=320)\n",
    "        if self.labels is not None:\n",
    "            enc[\"labels\"] = int(self.labels[i])\n",
    "        return enc\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Helpers.\n",
    "# --------------------------------------------------------------------------------------\n",
    "def logits_to_proba(logits):\n",
    "    \"\"\"Convert logits to P(class=1). Works for binary 1-logit or 2-logit outputs.\"\"\"\n",
    "    arr = np.asarray(logits)\n",
    "    if arr.ndim == 1 or arr.shape[-1] == 1:\n",
    "        z = arr.reshape(-1)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    # Stable softmax for 2-class case.\n",
    "    z = arr - arr.max(axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return (e[:, 1] / e.sum(axis=1)).astype(np.float64)\n",
    "\n",
    "def safe_logit(p, eps=1e-6):\n",
    "    \"\"\"Numerically stable logit.\"\"\"\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Train.\n",
    "# --------------------------------------------------------------------------------------\n",
    "oof_tr = np.zeros(len(train), dtype=float)\n",
    "test_pred_tr = np.zeros(len(test), dtype=float)\n",
    "auc_tr_folds = []\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "LR = 2e-5\n",
    "TRAIN_BS = 24 if IS_CUDA else 8\n",
    "EVAL_BS  = 48 if IS_CUDA else 16\n",
    "\n",
    "for fold, (tr, va) in enumerate(folds, 1):\n",
    "    ds_tr = DS(train[\"text_cond\"].iloc[tr].tolist(), y[tr])\n",
    "    ds_va = DS(train[\"text_cond\"].iloc[va].tolist(), y[va])\n",
    "    ds_ts = DS(test[\"text_cond\"].tolist(), None)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        torch_dtype=(torch.bfloat16 if USE_BF16 else torch.float16 if USE_FP16 else torch.float32),\n",
    "    )\n",
    "    device = \"cuda\" if IS_CUDA else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.gradient_checkpointing_disable()\n",
    "    print(f\"[Fold {fold}] Model device: {next(model.parameters()).device}, dtype: {next(model.parameters()).dtype}.\")\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./jigsaw_fold{fold}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"auc\",\n",
    "        greater_is_better=True,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        per_device_train_batch_size=TRAIN_BS,\n",
    "        per_device_eval_batch_size=EVAL_BS,\n",
    "        gradient_accumulation_steps=1,\n",
    "        fp16=USE_FP16,\n",
    "        bf16=USE_BF16,\n",
    "        tf32=True,                      # Safe NVIDIA speed boost.\n",
    "        no_cuda=not IS_CUDA,            # Force CUDA if available.\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=DATALOADER_WORKERS,\n",
    "        group_by_length=True,           # Less padding, faster.\n",
    "        gradient_checkpointing=False,   # Memory saver; Trainer will set use_cache=False.\n",
    "        logging_steps=100,\n",
    "        report_to=[],\n",
    "        save_total_limit=1,\n",
    "        seed=SEED,\n",
    "        data_seed=SEED,\n",
    "        auto_find_batch_size=False,     # Set True if you hit OOMs.\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        # Be robust to EvalPrediction or tuple.\n",
    "        logits = getattr(eval_pred, \"predictions\", None)\n",
    "        labels = getattr(eval_pred, \"label_ids\", None)\n",
    "        if logits is None or labels is None:\n",
    "            logits, labels = eval_pred\n",
    "        probs = logits_to_proba(logits)\n",
    "        return {\"auc\": roc_auc_score(labels, probs)}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds_tr,\n",
    "        eval_dataset=ds_va,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0001)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    val_logits = trainer.predict(ds_va).predictions\n",
    "    oof_tr[va] = logits_to_proba(val_logits)\n",
    "    auc = roc_auc_score(y[va], oof_tr[va])\n",
    "    auc_tr_folds.append(auc)\n",
    "\n",
    "    ts_logits = trainer.predict(ds_ts).predictions\n",
    "    test_pred_tr += logits_to_proba(ts_logits) / N_FOLDS\n",
    "\n",
    "    del trainer, model, ds_tr, ds_va, ds_ts\n",
    "    gc.collect()\n",
    "    if IS_CUDA:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"DeBERTa fold AUCs: {np.round(auc_tr_folds, 4)} | mean={np.mean(auc_tr_folds):.4f}.\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Calibration and blending.\n",
    "# --------------------------------------------------------------------------------------\n",
    "ARTIFACTS = Path(\"./artifacts\")\n",
    "def load_or_none(path):\n",
    "    return np.load(path) if path.exists() else None\n",
    "\n",
    "if \"oof_lr\" not in globals() or \"test_pred_lr\" not in globals():\n",
    "    oof_lr = load_or_none(ARTIFACTS / \"oof_lr.npy\")\n",
    "    test_pred_lr = load_or_none(ARTIFACTS / \"test_pred_lr.npy\")\n",
    "\n",
    "use_lr_view = (\n",
    "    (oof_lr is not None) and (test_pred_lr is not None)\n",
    "    and (len(oof_lr) == len(train)) and (len(test_pred_lr) == len(test))\n",
    ")\n",
    "\n",
    "if use_lr_view:\n",
    "    logit_lr = safe_logit(oof_lr)\n",
    "    logit_tr = safe_logit(oof_tr)\n",
    "    X_cal = np.vstack([logit_lr, logit_tr]).T\n",
    "    cal = LRCal(max_iter=200)\n",
    "    cal.fit(X_cal, y)\n",
    "    oof_blend = cal.predict_proba(X_cal)[:, 1]\n",
    "    print(f\"Blended OOF AUC: {roc_auc_score(y, oof_blend):.4f}.\")\n",
    "    test_logit_lr = safe_logit(test_pred_lr)\n",
    "    test_logit_tr = safe_logit(test_pred_tr)\n",
    "    X_test_blend = np.vstack([test_logit_lr, test_logit_tr]).T\n",
    "    test_pred = cal.predict_proba(X_test_blend)[:, 1]\n",
    "else:\n",
    "    # Fallback to calibrated transformer only.\n",
    "    X_cal = safe_logit(oof_tr).reshape(-1, 1)\n",
    "    cal = LRCal(max_iter=200).fit(X_cal, y)\n",
    "    oof_cal = cal.predict_proba(X_cal)[:, 1]\n",
    "    print(f\"Transformer-only calibrated OOF AUC: {roc_auc_score(y, oof_cal):.4f}.\")\n",
    "    test_pred = cal.predict_proba(safe_logit(test_pred_tr).reshape(-1, 1))[:, 1]\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Save submission.\n",
    "# --------------------------------------------------------------------------------------\n",
    "assert \"row_id\" in test.columns, \"Expected 'row_id' in test.csv.\"\n",
    "out = pd.DataFrame({\"row_id\": test[\"row_id\"], \"rule_violation\": test_pred})\n",
    "assert len(out) == len(test), \"Submission length must equal test length.\"\n",
    "assert np.isfinite(out[\"rule_violation\"]).all(), \"Predictions must be finite.\"\n",
    "out.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Wrote submission.csv with shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c734c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m SAVE_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeberta_v3_large_finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(SAVE_DIR)         \u001b[38;5;66;03m# Saves model weights + config.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(SAVE_DIR)  \u001b[38;5;66;03m# Saves tokenizer files.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"deberta_v3_large_finetuned\"\n",
    "trainer.save_model(SAVE_DIR)         # Saves model weights + config.\n",
    "tokenizer.save_pretrained(SAVE_DIR)  # Saves tokenizer files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c3e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
